---
format:
  html:
    title-block: false
    toc: false
---

<div style="text-align:center; padding:2em; font-family:Arial, sans-serif; height:100vh; display:flex; flex-direction:column; justify-content:space-between;">

<div style="display: flex; justify-content: space-between; align-items: center; width: 100%; padding: 0 2em;">
  <!-- Logo UM à gauche -->
  <img src="images/Universite.png" style="height: 100px;">

  <!-- Logo SSD-MIND à droite -->
  <img src="images/ssd_mind_logo.png" style="height: 100px;">
</div>

<!-- Titre central -->
<div>
# TP : Support Vector Machines
---
### Réalisé par :
**STETSUN Kateryna**  
**THOMAS Anne-Laure**  

Date : <span id="today"></span>
</div>

<div></div>
</div>

<script>
document.getElementById("today").textContent = new Date().toLocaleDateString("fr-FR");
</script>

# Introduction

Les machines à vecteurs de support (SVM), introduites par Vapnik dans les années 1990, sont devenues l'une des méthodes de classification supervisée les plus populaires. Leur succès s'explique par leur capacité à construire des règles de décision linéaires, appelées hyperplans séparateurs, tout en permettant d'aborder des problèmes où les données ne sont pas directement séparables dans l'espace d'entrée.

Le principe des SVM repose sur la recherche d'un hyperplan qui maximise la marge entre les classes. Dans le cas linéaire, cet hyperplan est directement construit dans l'espace des variables initiales. Dans les cas plus complexes, une transformation implicite des données dans un espace de dimension supérieure est effectuée grâce au `kernel trick`. Cette astuce permet de manipuler uniquement les produits scalaires dans l'espace transformé, en utilisant une fonction noyau. Parmi les noyaux courants, on retrouve le noyau linéaire, le noyau polynomial, le noyau gaussien RBF ou encore le noyau sigmoïde.

L'apprentissage consiste alors à résoudre un problème d'optimisation sous contraintes, dans lequel un paramètre de régularisation, noté "C", contrôle la complexité du modèle et le compromis entre maximisation de la marge et minimisation des erreurs de classification.

L'objectif de ce TP est de mettre en pratique ces concepts à travers différents jeux de données, simulés et réels. Nous utiliserons pour cela la bibliothèque `scikit-learn`, qui propose une implémentation performante des SVM via la librairie libsvm.

# Mise en oeuvre

## 1. Jeu de données jouet (deux gaussiennes)

```{python}
import numpy as np
import matplotlib.pyplot as plt
from sklearn.svm import SVC

from svm_source import *
from sklearn import svm
from sklearn import datasets
from sklearn.utils import shuffle
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.datasets import fetch_lfw_people
from sklearn.decomposition import PCA
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
from sklearn.metrics import classification_report
from time import time

scaler = StandardScaler()

import warnings
warnings.filterwarnings("ignore")

plt.style.use('ggplot')

n1 = 200
n2 = 200
mu1 = [1., 1.]
mu2 = [-1./2, -1./2]
sigma1 = [0.9, 0.9]
sigma2 = [0.9, 0.9]
np.random.seed(42)
X1, y1 = rand_bi_gauss(n1, n2, mu1, mu2, sigma1, sigma2)

plt.show()
plt.close("all")
plt.ion()
plt.figure(1, figsize=(15, 5))
plt.title('Premier ensemble de données')
plot_2d(X1, y1)

X_train = X1[::2]
Y_train = y1[::2].astype(int)
X_test = X1[1::2]
Y_test = y1[1::2].astype(int)

# ajuster le modèle avec un noyau linéaire
clf = SVC(kernel='linear')
clf.fit(X_train, Y_train)

# prédire les étiquettes pour la base de données de test
y_pred = clf.predict(X_test)

# vérifier votre score
score = clf.score(X_test, Y_test)
print('Score : %s' % score)
```

Le modèle SVM avec noyau linéaire obtient une performance de $0.905$ sur le jeu de données test. 

Le graphique obtenu représente un nuage de points issu d'un jeu de données synthétique généré par la fonction `rand_bi_gauss`. Celui-ci est tracé sur un plan cartésien dont l'axe des abscisses s'étend approximativement de $-3$ à $+3$, et l'axe des ordonnées de $-2$ à $+4$.
Deux groupes de points sont distingués par leur forme et leur couleur :
- Les points bleus, représentés par des cercles, sont majoritairement concentrés dans la partie inférieure gauche du graphique.
- Les points orange, représentés par des carrés, se regroupent dans la partie supérieure droite.
Cette séparation visuelle nette entre les deux ensembles suggère qu'ils correspondent à deux classes distinctes. Chaque point du graphique incarne une observation caractérisée par deux variables (coordonnées x et y), tandis que la couleur et la forme du marqueur indiquent son appartenance à une classe.

```{python}
# afficher la frontière
def f(xx):
    """Classificateur : nécessaire pour éviter les avertissements dus à des problèmes de forme"""
    return clf.predict(xx.reshape(1, -1))

plt.figure()
frontiere(f, X_train, Y_train, w=None, step=50, alpha_choice=1)
plt.title("Figure 1")

# Même procédure mais avec une recherche par grille
parameters = {'kernel': ['linear'], 'C': list(np.linspace(0.001, 3, 21))}
clf2 = SVC()
clf_grid = GridSearchCV(clf2, parameters, n_jobs=-1)
clf_grid.fit(X_train, Y_train)

# vérifiez votre score
print(clf_grid.best_params_)
print('Score : %s' % clf_grid.score(X_test, Y_test))

def f_grid(xx):
    """Classificateur : nécessaire pour éviter les avertissements dus à des problèmes de forme"""
    return clf_grid.predict(xx.reshape(1, -1))

# afficher la frontière
plt.figure()
frontiere(f_grid, X_train, Y_train, w=None, step=50, alpha_choice=1)
plt.title("Figure 2")
```

 Le modèle SVM améliorée est à $0.9$ après recherche de paramètre C optimal via GridSearch. Cela illustre l'importance de l'optimisation d'hyperparamètres pour améliorer la capacité de généralisation du modèle.

Les deux graphiques représentent la frontière de décision d'un SVM linéaire appliqué à un jeu de données synthétique.
Le premier (Figure 1) est issu d'un modèle entraîné sans optimisation, tandis que le second (Figure 2) résulte d'une validation croisée sur le paramètre de régularisation C.
Dans les deux cas, les zones colorées indiquent les prédictions du classifieur, et la séparation diagonale illustre la capacité du modèle à distinguer les deux classes.

## 2. Jeu de données iris

### Question 1 : Noyau linéaire

```{python}
iris = datasets.load_iris()
X = iris.data
X = scaler.fit_transform(X)
y = iris.target
X = X[y != 0, :2]
y = y[y != 0]

# test de train fractionné (par exemple, 25 % pour le test)
# Vous pouvez mélanger puis séparer, ou simplement utiliser train_test_split
# sans mélanger (dans ce cas, fixez l'état aléatoire (par exemple, à 42) pour la reproductibilité)
np.random.seed(42)
X, y = shuffle(X, y)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)
###############################################################################
# ajuster le modèle avec un noyau linéaire ou polynomial
###############################################################################

# ajuster le modèle et sélectionner le meilleur hyperparamètre C
parameters = {'kernel': ['linear'], 'C': list(np.logspace(-3, 3, 200))}
clf_linear = GridSearchCV(SVC(), parameters, cv=5)
clf_linear.fit(X_train, y_train)

# calculer le score

print('Score de généralisation pour le noyau linéaire : %s, %s' %
      (clf_linear.score(X_train, y_train),
       clf_linear.score(X_test, y_test)))


plt.figure(figsize=(15, 5))
plot_2d(X, y)
plt.title("Ensemble de données sur l'iris")
plt.show()

def f_linear(xx):
    return clf_linear.predict(xx.reshape(1, -1))

frontiere(f_linear, X, y)
plt.title("Noyau linéaire")
plt.tight_layout()
plt.draw()
```

Avec le jeu de données "iris" (classes 1 et 2, deux premières variables), l'utilisation d'un SVM à noyau linéaire donne une performance d'environ $0.69$ sur l'échantillon test. 
Après optimisation du paramètre de régularisation C via une recherche sur grille (GridSearchCV), le score de généralisation s'élève à $0.72$.

Cette amélioration illustre le rôle crucial du paramètre C, qui contrôle le compromis entre la largeur de la marge et le taux d'entraînement.

Le premier graphique représente les données du jeu Iris. Les points sont codés par couleur et forme selon leur classe, révélant un chevauchement partiel entre les deux groupes.
Le second graphique illustre la frontière de décision d'un SVM linéaire optimisé. Les zones colorées indiquent les prédictions du modèle, et la séparation diagonale montre la capacité du classifieur à distinguer les deux classes dans cet espace réduit

### Question 2 : Noyau polynomial 

```{python}
Cs = list(np.logspace(-3, 3, 5))
gammas = 10. ** np.arange(1, 2)
degrees = np.r_[1, 2, 3]

# ajuster le modèle et sélectionner le meilleur ensemble d'hyperparamètres
parameters = {'kernel': ['poly'], 'C': Cs, 'gamma': gammas, 'degree': degrees}
clf_poly = GridSearchCV(SVC(), parameters, cv=5)
clf_poly.fit(X_train, y_train)


print(clf_grid.best_params_)
print('Score de généralisation pour le noyau polynomial: %s, %s' %
      (clf_poly.score(X_train, y_train),
       clf_poly.score(X_test, y_test)))

# affichez vos résultats en utilisant frontiere (svm_source.py)

def f_poly(xx):
    return clf_poly.predict(xx.reshape(1, -1))

frontiere(f_poly, X, y)
plt.title("Noyau polynomial")
plt.tight_layout()
plt.draw()
```

En utilisant un SVM à noyau polynomial et en ajustant le paramètre de régularisation C, les performances atteignent environ $0,69$ sans optimisation, et $0,84$ après recherche de l'hyperparamètre optimal.

Le graphique ci-dessus illustre la frontière de décision non linéaire obtenue avec ce noyau, mieux adaptée à la complexité du jeu de données Iris. Contrairement au noyau linéaire, qui produit une séparation rectiligne, le noyau polynomial permet de capturer plus finement les zones de chevauchement entre les classes, améliorant ainsi la capacité de discrimination dans un espace réduit.

La comparaison montre que le noyau polynomial offre une meilleure généralisation, grâce à sa flexibilité. 

# 3. Classification de visages

Pour notre analyse, nous avons chargé l'ensemble de données en utilisant le script "svm_script.py". 
Nous avons apporté une petite modification à la ligne de code originale pour améliorer la compatibilité et la clarté :  
la ligne 
```text
y = np.r_[np.zeros(np.sum(idx0)), np.ones(np.sum(idx1))].astype(np.int)
```
a été remplacée par
```text
y = np.r_[np.zeros(np.sum(idx0)), np.ones(np.sum(idx1))].astype(int)
```
.

Cette modification correspond à la nouvelle version de l’extraction des données.

Voici la version finale du code qui a été employée pour charger l’ensemble de données :

```{python}
"""
The dataset used in this example is a preprocessed excerpt
of the "Labeled Faces in the Wild", aka LFW_:
  http://vis-www.cs.umass.edu/lfw/lfw-funneled.tgz (233MB)
  _LFW: http://vis-www.cs.umass.edu/lfw/
"""

# Download the data and unzip; then load it as numpy arrays
lfw_people = fetch_lfw_people(min_faces_per_person=70, resize=0.4,
                              color=True, funneled=False, slice_=None,
                              download_if_missing=True)
# data_home='.'

# introspect the images arrays to find the shapes (for plotting)
images = lfw_people.images
n_samples, h, w, n_colors = images.shape

# the label to predict is the id of the person
target_names = lfw_people.target_names.tolist()

# Pick a pair to classify such as
# names = ['Donald Rumsfeld', 'Colin Powell']
names = ['Tony Blair', 'Colin Powell']

idx0 = (lfw_people.target == target_names.index(names[0]))
idx1 = (lfw_people.target == target_names.index(names[1]))
images = np.r_[images[idx0], images[idx1]]
n_samples = images.shape[0]
y = np.r_[np.zeros(np.sum(idx0)), np.ones(np.sum(idx1))].astype(int)

# plot a sample set of the data
plot_gallery(images, np.arange(12))
plt.show()

# Extract features
# features using only illuminations
X = (np.mean(images, axis=3)).reshape(n_samples, -1)
# Scale features
X -= np.mean(X, axis=0)
X /= np.std(X, axis=0)

# Split data into a half training and half test set

indices = np.random.permutation(X.shape[0])
train_idx, test_idx = indices[:X.shape[0] // 2], indices[X.shape[0] // 2:]
X_train, X_test = X[train_idx, :], X[test_idx, :]
y_train, y_test = y[train_idx], y[test_idx]
images_train, images_test = images[
    train_idx, :, :, :], images[test_idx, :, :, :]
```
Dans ce bloc de code, nous chargeons et préparons l’ensemble de données **Labeled Faces in the Wild (LFW)**.  
Nous commençons par télécharger les images, puis nous sélectionnons uniquement deux individus à distinguer : *Tony Blair* et *Colin Powell*.  

Ensuite, nous associons à chaque image une étiquette binaire (0 pour Tony Blair et 1 pour Colin Powell), et nous visualisons un échantillon de visages afin d’illustrer le jeu de données.  

Les caractéristiques utilisées correspondent aux **niveaux de luminosité moyens** des pixels de chaque image, ce qui permet de réduire la complexité de la représentation. Ces variables sont ensuite **centrées et réduites** pour normaliser les données.  

Enfin, nous divisons l’ensemble en deux parties : **un jeu d’entraînement** et **un jeu de test**, chacun contenant environ la moitié des observations. Cela garantit une évaluation fiable de la performance du classificateur.

### Question 4 - Influence du paramètre de réguralisation C

Pour étudier l'influence du paramètre de régularisation $C$, nous avons utilisé un classificateur SVM à noyau linéaire. 
Nous avons testé plusieurs valeurs de $C$ réparties logarithmiquement entre $$10^{-5} \quad \text{et} \quad 10^{5}.$$ 

```{python}
print("--- Linear kernel ---")
print("Fitting the classifier to the training set")
t0 = time()

# fit a classifier (linear) and test all the Cs
Cs = 10. ** np.arange(-5, 6)
scores = []
for C in Cs:
    clf = SVC(kernel="linear", C=C)
    clf.fit(X_train, y_train)
    scores.append(clf.score(X_test, y_test))

ind = np.argmax(scores)
print("Best C: {}".format(Cs[ind]))

plt.figure()
plt.plot(Cs, scores)
plt.xlabel("Parametres de regularisation C")
plt.ylabel("Scores d'apprentissage")
plt.xscale("log")
plt.tight_layout()
plt.show()
print("Best score: {}".format(np.max(scores)))

print("Predicting the people names on the testing set")
t0 = time()
```

L'étude de la performance en fonction de C permet de choisir une valeur optimale de régularisation, minimisant l'erreur et garantissant un apprentissage équilibré.

Nous pouvons également visualiser l'influence du paramètre de régularisation \(C\) sur l'erreur de prédiction. 

Pour cela, nous construisons un graphique similaire au précédent, mais en affichant l'erreur plutôt que le score d'apprentissage. 
La meilleure valeur de \(C\) est indiquée par un point rouge sur le graphique.

```{python}
# calculons l`erreur
errors = []
for C in Cs:
    clf = SVC(kernel="linear", C=C)
    clf.fit(X_train, y_train)
    errors.append(1 - clf.score(X_test, y_test))

# cherche min erreur
best_ind = np.argmin(errors)
best_C = Cs[best_ind]
best_error = errors[best_ind]

plt.figure()
plt.plot(Cs, errors, marker="o")

# plot erreue
plt.scatter(best_C, best_error, color="red", s=100, zorder=5)

plt.xscale("log")
plt.xlabel("Paramètre de régularisation C")
plt.ylabel("Erreur de prédiction")
plt.title("Influence de C sur la performance")
plt.grid(True)
plt.tight_layout()
plt.show()

print("Best C: {}".format(Cs[best_ind]))
print("Best error: {}".format(np.min(errors)))
print("Best accuracy(score): {}".format(1 - np.min(errors)))
t0 = time()
```

Ces deux graphiques sont complémentaires et montrent de manière claire que :  
    - pour des valeurs de \(C\) trop faibles, le modèle est *sous-appris* (underfitting),  
    - pour des valeurs de \(C\) trop élevées, le modèle est *sur-appris* (overfitting).  

Pour continuer l'analyse de l'effet du paramètre de régularisation \(C\), nous construisons également les matrices de confusion pour deux valeurs extrêmes de \(C\): une très petite ($C=1e^{-5}$) et une très grande ($C=1e^{5}$).

La **matrice de confusion** permet de visualiser la performance du classificateur de manière détaillée. 
Chaque ligne correspond aux classes réelles, et chaque colonne aux classes prédites.  

Ainsi, elle montre non seulement le taux global de bonne classification, mais aussi quelles classes sont confondues par le modèle.

```{python}
# confusion matrix
#small C
clf_small = SVC(kernel="linear", C=1e-5)
clf_small.fit(X_train, y_train)
y_pred_small = clf_small.predict(X_test)

cm_small = confusion_matrix(y_test, y_pred_small, labels=clf_small.classes_)
disp_small = ConfusionMatrixDisplay(confusion_matrix=cm_small, display_labels=clf_small.classes_)

# big C
clf_large = SVC(kernel="linear", C=1e5)
clf_large.fit(X_train, y_train)
y_pred_large = clf_large.predict(X_test)

cm_large = confusion_matrix(y_test, y_pred_large, labels=clf_large.classes_)
disp_large = ConfusionMatrixDisplay(confusion_matrix=cm_large, display_labels=clf_large.classes_)

# rows
fig, axes = plt.subplots(1, 2, figsize=(12, 5))
# 0 = Tony Blair, 1 = Colin Powell)
class_names = ['Tony Blair', 'Colin Powell']

disp_small = ConfusionMatrixDisplay(confusion_matrix=cm_small,
                                    display_labels=class_names)
disp_small.plot(ax=axes[0], cmap="Blues", xticks_rotation='vertical', colorbar=False)
axes[0].set_title("Confusion matrix (C=1e-5)")
disp_large = ConfusionMatrixDisplay(confusion_matrix=cm_large,
                                    display_labels=class_names)
disp_large.plot(ax=axes[1], cmap="Blues", xticks_rotation='vertical', colorbar=False)
axes[1].set_title("Confusion matrix (C=1e5)")

plt.tight_layout()
plt.show()
```

Nous fournirons les mêmes résultats, mais sous forme numérique

```{python}
print("=== Classification report (C=1e-5) ===")
print(classification_report(y_test, y_pred_small, target_names=class_names))

print("\n=== Classification report (C=1e5) ===")
print(classification_report(y_test, y_pred_large, target_names=class_names))
```

Sur la matrice de confusion, nous pouvons observer que, pour une valeur très petite de \(C = 10^{-5}\), le modèle prédit systématiquement la classe *Colin Powell*, sans jamais identifier correctement *Tony Blair*. En revanche, pour des valeurs plus grandes de \(C\), le modèle devient plus précis, même si certaines erreurs persistent.  

L’analyse des rapports de classification confirme cette observation :  
- Avec \(C = 10^{-5}\), la précision et le rappel pour *Tony Blair* sont nuls, ce qui signifie que le modèle est totalement incapable de reconnaître cette classe.  
- Avec \(C = 10^{5}\), la performance s’améliore nettement avec une précision globale de 88 %, bien que quelques erreurs subsistent pour les deux individus.  

Ainsi, on peut conclure que des valeurs de \(C\) trop faibles entraînent un sous-apprentissage, tandis que des valeurs très élevées permettent au modèle de mieux capturer les différences entre les classes, mais ne suppriment pas totalement les erreurs de classification.

Nous comparons la précision du modèle avec le niveau de hasard (baseline). Ainsi, nous pouvons évaluer dans quelle mesure la valeur choisie de C améliore les résultats.

```{python}
# predict labels for the X_test images with the best classifier
clf = SVC(kernel="linear", C=Cs[ind])
clf.fit(X_train, y_train)
y_pred = clf.predict(X_test)

print("done in %0.3fs" % (time() - t0))
# The chance level is the accuracy that will be reached when constantly predicting the majority class.
print("Chance level : %s" % max(np.mean(y), 1. - np.mean(y)))
print("Accuracy : %s" % clf.score(X_test, y_test))
```

Le résultat obtenu signifie que :
    - l’entraînement du modèle a duré 0,697 seconde,
    - le niveau de hasard est de 62,1 % (c’est-à-dire la précision atteinte si l’on prédit toujours la classe majoritaire),
    - et la précision réelle du modèle sur les données de test est de 90,5 %, ce qui confirme l’efficacité du paramètre de régularisation choisi.»

Nous avons aussi comparé les prédictions et les coefficients du classificateur pour trois cas : la meilleure valeur de \(C\), la valeur très faible (\(C=10^{-5}\)) et la valeur très grande (\(C=10^{5}\)).

Pour le meilleur \(C\), le code suivant permet de prédire les étiquettes sur l'ensemble de test, d'afficher les images avec les prédictions et de visualiser les coefficients du classificateur :

```{python}
####################################################################
# Qualitative evaluation of the predictions using matplotlib

prediction_titles = [title(y_pred[i], y_test[i], names)
                     for i in range(y_pred.shape[0])]

plot_gallery(images_test, prediction_titles)
plt.show()

####################################################################
# Look at the coefficients
plt.figure()
plt.imshow(np.reshape(clf.coef_, (h, w)))
plt.show()
```

Pour montrer l’effet du paramètre \(C\), nous avons également entraîné des modèles pour les valeurs extrêmes.

    - Pour $C = 10^{-5}$, l’image des coefficients est très floue, et seules les caractéristiques les plus importantes sont prises en compte, telles que la taille du front, les sourcils, la forme du nez et des lèvres.
    - Pour $C = 10^{-5}$, le modèle est sur-appris et commence à détecter des motifs même là où il n’y en a pas. Par exemple, on observe beaucoup de points rouges sur l’arrière-plan des images, ce qui n’est pas pertinent, car les deux personnes peuvent être photographiées dans n’importe quelle pièce.

```{python}
# Look at the coefficients pour C=1e-5 et C=1e5
coef_small = clf_small.coef_.ravel()
coef_large = clf_large.coef_.ravel()

fig, axes = plt.subplots(1, 2, figsize=(10, 5))

axes[0].imshow(coef_small.reshape(h, w), cmap=plt.cm.seismic, interpolation="nearest")
axes[0].set_title("Coefficients (C=1e-5)")

axes[1].imshow(coef_large.reshape(h, w), cmap=plt.cm.seismic, interpolation="nearest")
axes[1].set_title("Coefficients (C=1e5)")

plt.tight_layout()
plt.show()
```
Cette comparaison illustre clairement la différence entre **sous-apprentissage** et **sur-apprentissage** et montre comment le paramètre \(C\) contrôle la sensibilité du modèle aux caractéristiques des données.

### Question 5 - Ajout de variables de nuisance

Dans cette partie, nous étudions l’effet de l’ajout de variables de nuisance (c’est-à-dire des variables aléatoires sans lien avec la tâche de classification). L’idée est d’augmenter artificiellement la dimension du problème tout en gardant le même nombre de points d’apprentissage. 

```{python}
def run_svm_cv(_X, _y):
    _indices = np.random.permutation(_X.shape[0])
    _train_idx, _test_idx = _indices[:_X.shape[0] // 2], _indices[_X.shape[0] // 2:]
    _X_train, _X_test = _X[_train_idx, :], _X[_test_idx, :]
    _y_train, _y_test = _y[_train_idx], _y[_test_idx]

    _parameters = {'kernel': ['linear'], 'C': list(np.logspace(-3, 3, 5))}
    _svr = svm.SVC()
    _clf_linear = GridSearchCV(_svr, _parameters)
    _clf_linear.fit(_X_train, _y_train)

    print('Generalization score for linear kernel: %s, %s \n' %
          (_clf_linear.score(_X_train, _y_train), _clf_linear.score(_X_test, _y_test)))

print("Score sans variable de nuisance")
run_svm_cv(X, y)

print("Score avec variable de nuisance")
n_features = X.shape[1]
# On rajoute des variables de nuisances
sigma = 1
noise = sigma * np.random.randn(n_samples, 300, ) 
#with gaussian coefficients of std sigma
X_noisy = np.concatenate((X, noise), axis=1)
X_noisy = X_noisy[np.random.permutation(X.shape[0])]
np.random.shuffle(X_noisy.T)

run_svm_cv(X_noisy, y)
```

Les expériences confirment cette intuition. Sans ajout de variables de nuisance, le SVM atteint une excellente performance, avec une précision d’environ 0.89 sur l’échantillon de test. En revanche, après l’ajout de 300 variables aléatoires, la performance chute drastiquement : la précision sur l’échantillon de test tombe à environ 0.48, soit proche d’un choix aléatoire. Ce contraste illustre clairement que l’ajout de variables inutiles provoque un surapprentissage du modèle et dégrade sa capacité de généralisation.

Pour rendre les résultats plus clairs et reproductibles, nous fixons `random_state=42` et examinons l’effet du nombre de variables aléatoires (bruit) sur l’échantillon de test. Le graphique suivant montre cette relation.  

```{python}
def run_svm_cv(_X, _y, random_state=42):
    rng = np.random.RandomState(random_state)
    indices = rng.permutation(_X.shape[0])
    train_idx, test_idx = indices[:_X.shape[0] // 2], indices[_X.shape[0] // 2:]
    X_train, X_test = _X[train_idx, :], _X[test_idx, :]
    y_train, y_test = _y[train_idx], _y[test_idx]

    parameters = {'kernel': ['linear'], 'C': list(np.logspace(-3, 3, 5))}
    svr = svm.SVC()
    clf = GridSearchCV(svr, parameters)
    clf.fit(X_train, y_train)

    return clf.score(X_test, y_test)

noise_dims = np.arange(10, 1001, 25)
test_scores = []

rng = np.random.RandomState(42)
for n_noise in noise_dims:
    noise = rng.randn(X.shape[0], n_noise)
    X_aug = np.concatenate((X, noise), axis=1)
    score = run_svm_cv(X_aug, y)
    test_scores.append(score)

# Построение графика
plt.figure(figsize=(10,6))
plt.plot(noise_dims, test_scores, color='red')
plt.title("Influence du nombre de variables de nuisance sur l'accuracy")
plt.xlabel("Nombre de variables de nuisance")
plt.ylabel("Accuracy sur l'échantillon de test")
plt.grid(True)
plt.show()
```

On peut remarquer que la précision reste globalement élevée, ce qui indique que le modèle continue à exploiter les informations pertinentes des caractéristiques originales. Néanmoins, comme prévu, on observe une diminution progressive de la performance à mesure que le nombre de variables de nuisance augmente, illustrant l’effet négatif de l’ajout de dimensions non informatives.

### Question 6 - PCA


si on utilise les code 

```text
import time as tm
print("Score apres reduction de dimension")

n_components = 10  # jouer avec ce parametre
pca = PCA(n_components=n_components).fit(X_noisy)
X_pca = pca.transform(X_noisy)

# timing
t0 = tm.time()
run_svm_cv(X_pca, y)
elapsed = tm.time() - t0

print(f"Nombre de composantes PCA: {n_components}")
print(f"Temps de calcul: {elapsed:.3f} secondes")
```
sortie de code :


Score apres reduction de dimension
Generalization score for linear kernel: 0.6526315789473685, 0.5894736842105263 

Nombre de composantes PCA: 2
Temps de calcul: 173.942 secondes

Score apres reduction de dimension
Generalization score for linear kernel: 0.6421052631578947, 0.6 

Nombre de composantes PCA: 10
Temps de calcul: 733.982 secondes

Score apres reduction de dimension
Generalization score for linear kernel: 0.6842105263157895, 0.6052631578947368 

Nombre de composantes PCA: 20
Temps de calcul: 1357.474 secondes

Score apres reduction de dimension
Generalization score for linear kernel: 0.8789473684210526, 0.5736842105263158 

Score apres reduction de dimension
Generalization score for linear kernel: 0.7736842105263158, 0.5473684210526316 

Nombre de composantes PCA: 50
Temps de calcul: 2886.425 secondes

Nombre de composantes PCA: 100
Temps de calcul: 0.145 secondes

Score apres reduction de dimension
Generalization score for linear kernel: 0.868421052631579, 0.631578947368421 

Nombre de composantes PCA: 150
Temps de calcul: 0.095 secondes

Score apres reduction de dimension
Generalization score for linear kernel: 1.0, 0.5 

Nombre de composantes PCA: 200
Temps de calcul: 0.101 secondes

### Question 7 - Biais dans le prétaitement
