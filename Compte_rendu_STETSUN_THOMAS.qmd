---
format:
  html:
    title-block: false
    toc: false
---

<div style="text-align:center; padding:2em; font-family:Arial, sans-serif; height:100vh; display:flex; flex-direction:column; justify-content:space-between;">

<div style="display: flex; justify-content: space-between; align-items: center; width: 100%; padding: 0 2em;">
  <!-- Logo UM à gauche -->
  <img src="images/Universite.png" style="height: 100px;">

  <!-- Logo SSD-MIND à droite -->
  <img src="images/ssd_mind_logo.png" style="height: 100px;">
</div>

<!-- Titre central -->
<div>
# TP : Support Vector Machines
---
### Réalisé par :
**STETSUN Kateryna**  
**THOMAS Anne-Laure**  

Date : <span id="today"></span>
</div>

<div></div>
</div>

<script>
document.getElementById("today").textContent = new Date().toLocaleDateString("fr-FR");
</script>

# Introduction

Les machines à vecteurs de support (SVM), introduites par Vapnik dans les années 1990, sont devenues l'une des méthodes de classification supervisée les plus populaires. Leur succès s'explique par leur capacité à construire des règles de décision linéaires, appelées hyperplans séparateurs, tout en permettant d'aborder des problèmes où les données ne sont pas directement séparables dans l'espace d'entrée.

Le principe des SVM repose sur la recherche d'un hyperplan qui maximise la marge entre les classes. Dans le cas linéaire, cet hyperplan est directement construit dans l'espace des variables initiales. Dans les cas plus complexes, une transformation implicite des données dans un espace de dimension supérieure est effectuée grâce au `kernel trick`. Cette astuce permet de manipuler uniquement les produits scalaires dans l'espace transformé, en utilisant une fonction noyau. Parmi les noyaux courants, on retrouve le noyau linéaire, le noyau polynomial, le noyau gaussien RBF ou encore le noyau sigmoïde.

L'apprentissage consiste alors à résoudre un problème d'optimisation sous contraintes, dans lequel un paramètre de régularisation, noté "C", contrôle la complexité du modèle et le compromis entre maximisation de la marge et minimisation des erreurs de classification.

L'objectif de ce TP est de mettre en pratique ces concepts à travers différents jeux de données, simulés et réels. Nous utiliserons pour cela la bibliothèque `scikit-learn`, qui propose une implémentation performante des SVM via la librairie libsvm.

# Mise en oeuvre

## 1. Jeu de données jouet (deux gaussiennes)

```{python}
# Chargement des bibliothèques
import numpy as np                  # pour la manipulation de tableaux et la génération des données
import matplotlib.pyplot as plt     # pour les visualisations
from sklearn.svm import SVC         # le classifieur SVM et scikit-learn

# Importations supplémentaires 
from svm_source import *            # importe toutes les fonctions définies dans un fichier externe svm_source.py
from sklearn import svm
from sklearn import datasets
from sklearn.utils import shuffle
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.datasets import fetch_lfw_people
from sklearn.decomposition import PCA
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
from sklearn.metrics import classification_report
from time import time

# Préparation de l'environnement
scaler = StandardScaler()

# Initialise un standardisateur
import warnings
warnings.filterwarnings("ignore")

# Supprime les avertissements
plt.style.use('ggplot')

# Génération de données simulées – Créer deux classes de données gaussiennes
# Nombre d'échantillons par classe
n1 = 200               
n2 = 200
# Moyennes des distributions 
mu1 = [1., 1.]
mu2 = [-1./2, -1./2]
# Écarts-types
sigma1 = [0.9, 0.9]
sigma2 = [0.9, 0.9]
# Fixe la graine pour la reproductibilité
np.random.seed(42)
# Génère les données
X1, y1 = rand_bi_gauss(n1, n2, mu1, mu2, sigma1, sigma2)

# Visualisations des données
plt.show()
plt.close("all")
plt.ion()
plt.figure(1, figsize=(15, 5))
plt.title('Premier ensemble de données')
plot_2d(X1, y1)

# Séparation des données et conversion explicite des labels en entiers
X_train = X1[::2]
Y_train = y1[::2].astype(int)
X_test = X1[1::2]
Y_test = y1[1::2].astype(int)

# Entraînement du modèle SV
# ajuste le modèle avec un noyau linéaire
clf = SVC(kernel='linear')
clf.fit(X_train, Y_train)

# Prédire les étiquettes pour la base de données de test
y_pred = clf.predict(X_test)

# Vérifier le score obtenu
score = clf.score(X_test, Y_test)
print('Score : %s' % score)
```

Le modèle SVM avec noyau linéaire obtient une performance de $0.905$ sur le jeu de données test. 

Le graphique obtenu représente un nuage de points issu d'un jeu de données synthétique généré par la fonction `rand_bi_gauss`. Celui-ci est tracé sur un plan cartésien dont l'axe des abscisses s'étend approximativement de $-3$ à $+3$, et l'axe des ordonnées de $-2$ à $+4$.
Deux groupes de points sont distingués par leur forme et leur couleur :
- Les points bleus, représentés par des cercles, sont majoritairement concentrés dans la partie inférieure gauche du graphique.
- Les points orange, représentés par des carrés, se regroupent dans la partie supérieure droite.
Cette séparation visuelle nette entre les deux ensembles suggère qu'ils correspondent à deux classes distinctes. Chaque point du graphique incarne une observation caractérisée par deux variables (coordonnées x et y), tandis que la couleur et la forme du marqueur indiquent son appartenance à une classe.

```{python}
# Afficher la frontière de décision du SVM entrainé
def f(xx):
    """Classificateur : nécessaire pour éviter les avertissements dus à des problèmes de forme"""
    return clf.predict(xx.reshape(1, -1))

# Créer une nouvelle figure 
plt.figure()
frontiere(f, X_train, Y_train, w=None, step=50, alpha_choice=1)
plt.title("Figure 1")

# Même procédure mais avec une recherche par grille
parameters = {'kernel': ['linear'], 'C': list(np.linspace(0.001, 3, 21))}
clf2 = SVC()
clf_grid = GridSearchCV(clf2, parameters, n_jobs=-1)
clf_grid.fit(X_train, Y_train)

# Vérifiez le score obtenu
print(clf_grid.best_params_)
print('Score : %s' % clf_grid.score(X_test, Y_test))

def f_grid(xx):
    """Classificateur : nécessaire pour éviter les avertissements dus à des problèmes de forme"""
    return clf_grid.predict(xx.reshape(1, -1))

# Afficher la nouvelle frontière de décision du modèle optimisé
plt.figure()
frontiere(f_grid, X_train, Y_train, w=None, step=50, alpha_choice=1)
plt.title("Figure 2")
```

 Le modèle SVM améliorée est à $0.9$ après recherche de paramètre C optimal via GridSearch. Cela illustre l'importance de l'optimisation d'hyperparamètres pour améliorer la capacité de généralisation du modèle.

Les deux graphiques représentent la frontière de décision d'un SVM linéaire appliqué à un jeu de données synthétique.
Le premier (Figure 1) est issu d'un modèle entraîné sans optimisation, tandis que le second (Figure 2) résulte d'une validation croisée sur le paramètre de régularisation C.
Dans les deux cas, les zones colorées indiquent les prédictions du classifieur, et la séparation diagonale illustre la capacité du modèle à distinguer les deux classes.

## 2. Jeu de données iris

### Question 1 : Noyau linéaire

```{python}
# Chargement et préparation des données Iris 
iris = datasets.load_iris()
X = iris.data               # Contient les 4 caractéristiques
X = scaler.fit_transform(X) # Standardise les données (centrées réduites)
y = iris.target             # Contient les étiquettes de classes
# Filtrage des classes et réduction dimensionnelle
X = X[y != 0, :2]           # Supprime la classe 0 (Setosa) pour n'en garder que 2
y = y[y != 0]

# Test de train fractionné 75 % entrainement et 25 % pour le test

X, y = shuffle(X, y)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)

###############################################################################
# Ajuster le modèle avec un noyau linéaire ou polynomial
###############################################################################

# Recherche du meilleur hyperparamètre C pour un noyau linéaire 
parameters = {'kernel': ['linear'], 'C': list(np.logspace(-3, 3, 200))}
clf_linear = GridSearchCV(SVC(), parameters, cv=5)
clf_linear.fit(X_train, y_train)

# Évaluation du modèle avec le calcul du score
print('Score de généralisation pour le noyau linéaire : %s, %s' %
      (clf_linear.score(X_train, y_train),
       clf_linear.score(X_test, y_test)))

# Visualisation des données et de la frontière de décision
plt.figure(figsize=(15, 5))
plot_2d(X, y)
plt.title("Ensemble de données sur l'iris")
plt.show()

# Fonction utilitaire pour prédire un point
def f_linear(xx):
    return clf_linear.predict(xx.reshape(1, -1))

# Affichage de la frontière de décision du modèle optimisé
frontiere(f_linear, X, y)
plt.title("Noyau linéaire")
plt.tight_layout()
plt.draw()
```

Avec le jeu de données "iris" (classes 1 et 2, deux premières variables), l'utilisation d'un SVM à noyau linéaire donne une performance d'environ $0.68$ sur l'échantillon test. 
Après optimisation du paramètre de régularisation C via une recherche sur grille (GridSearchCV), le score de généralisation s'élève à environ $0.70$.

Cette amélioration illustre le rôle crucial du paramètre C, qui contrôle le compromis entre la largeur de la marge et le taux d'entraînement.

Le premier graphique représente les données du jeu Iris. Les points sont codés par couleur et forme selon leur classe, révélant un chevauchement partiel entre les deux groupes.
Le second graphique illustre la frontière de décision d'un SVM linéaire optimisé. Les zones colorées indiquent les prédictions du modèle, et la séparation diagonale montre la capacité du classifieur à distinguer les deux classes dans cet espace réduit

### Question 2 : Noyau polynomial 

```{python}
# Définition des hyperparamètres pour le noyau polynomial
Cs = list(np.logspace(-3, 3, 5))
gammas = 10. ** np.arange(1, 2)
degrees = np.r_[1, 2, 3]

# Recherche du meilleur ensemble d'hyperparamètres
parameters = {'kernel': ['poly'], 'C': Cs, 'gamma': gammas, 'degree': degrees}
clf_poly = GridSearchCV(SVC(), parameters, cv=5)
clf_poly.fit(X_train, y_train)

# Évaluation du modèle avec le calcul du score
print(clf_poly.best_params_)
print('Score de généralisation pour le noyau polynomial: %s, %s' %
      (clf_poly.score(X_train, y_train),
       clf_poly.score(X_test, y_test)))

# Fonction utilitaire pour prédire un point avec le modèle polynomial
def f_poly(xx):
    return clf_poly.predict(xx.reshape(1, -1))

# Visualisation de la frontière de décision
frontiere(f_poly, X, y)
plt.title("Noyau polynomial")
plt.tight_layout()
plt.draw()
```

En utilisant un SVM à noyau polynomial et en ajustant le paramètre de régularisation C, les performances atteignent environ $0,72$ sans optimisation, et $0,76$ après recherche de l'hyperparamètre optimal.

Le graphique ci-dessus illustre la frontière de décision non linéaire obtenue avec ce noyau, mieux adaptée à la complexité du jeu de données Iris. Contrairement au noyau linéaire, qui produit une séparation rectiligne, le noyau polynomial permet de capturer plus finement les zones de chevauchement entre les classes, améliorant ainsi la capacité de discrimination dans un espace réduit.

La comparaison montre que le noyau polynomial offre une meilleure généralisation, grâce à sa flexibilité. 

# 3. Classification de visages

Pour notre analyse, nous avons utilisé un extrait prétraité du jeu de données "Labeled Faces in the Wild" (LFW), qui contient des images de visages de célébrités. Nous avons sélectionné uniquement deux individus (*Tony Blair* et Colin Powell) afin de réaliser une classification binaire.

Pour améliorer la compatibilité et la clarté, nous avons apporté une petite modification à la ligne de code originale se trouvant dans `svm_script.py` :  

La ligne 
```text
y = np.r_[np.zeros(np.sum(idx0)), np.ones(np.sum(idx1))].astype(np.int)
```
a été remplacée par
```text
y = np.r_[np.zeros(np.sum(idx0)), np.ones(np.sum(idx1))].astype(int)
```

Voici la version finale du code qui a été employée pour charger l'ensemble des données :

```{python}
# Chargement du jeu de données LFW
lfw_people = fetch_lfw_people(min_faces_per_person=70, resize=0.4,
                              color=True, funneled=False, slice_=None,
                              download_if_missing=True)

# Extraction des images et dimensions
images = lfw_people.images
n_samples, h, w, n_colors = images.shape

# L'étiquette à prédire est l'identifiant de la personne
target_names = lfw_people.target_names.tolist()

# Nous avons choisi ici une paire à classer : 'Donald Rumsfeld' et 'Colin Powell'
names = ['Donald Rumsfeld', 'Colin Powell']

# Filtrage des images pour les deux classes
idx0 = (lfw_people.target == target_names.index(names[0]))
idx1 = (lfw_people.target == target_names.index(names[1]))
images = np.r_[images[idx0], images[idx1]]
n_samples = images.shape[0]
y = np.r_[np.zeros(np.sum(idx0)), np.ones(np.sum(idx1))].astype(int)

# Visualisation d'un échantillon de données
plot_gallery(images, np.arange(12))
plt.show()

# Extraire des caractéristiques
X = (np.mean(images, axis=3)).reshape(n_samples, -1)
# Centrage et réduction des caractéristiques pour normaliser les données
X -= np.mean(X, axis=0)
X /= np.std(X, axis=0)

# Séparation en train/test
indices = np.random.permutation(X.shape[0])
train_idx, test_idx = indices[:X.shape[0] // 2], indices[X.shape[0] // 2:]
X_train, X_test = X[train_idx, :], X[test_idx, :]
y_train, y_test = y[train_idx], y[test_idx]
images_train, images_test = images[
    train_idx, :, :, :], images[test_idx, :, :, :]
```

Après avoir chargé et préparé l'ensemble de données, nous avons conservé uniquement les images de *Donald Rumsfeld* et *Colin Powell* afin de simplifier le problème en une classification binaire. Chaque image reçoit une étiquette correspondant à son individu : 0 pour *Donald Rumsfeld* et 1 pour *Colin Powell*.

Pour illustrer la diversité des visages et la qualité des images utilisées, un échantillon de $12$ images est affiché. Les caractéristiques extraites pour chaque image correspondent à la moyenne des intensités lumineuses des pixels sur les trois canaux de couleur, ce qui permet de réduire la complexité des données tout en conservant les informations essentielles pour la classification.

Afin de faciliter l'apprentissage du classificateur, ces caractéristiques sont centrées et réduites, garantissant que chaque variable a une moyenne nulle et un écart-type égal à un. Enfin, l'ensemble de données est divisé de manière aléatoire en deux parties : un jeu d'entraînement et un jeu de test, contenant chacun environ la moitié des observations. Cette séparation permet d'évaluer de manière fiable les performances du modèle sur des images jamais vues pendant l'entraînement.

### Question 4 - Influence du paramètre de régularisation C

Pour étudier l'influence du paramètre de régularisation C, nous avons utilisé un classificateur SVM à noyau linéaire. Ce paramètre contrôle l'équilibre entre la maximisation de la marge et la pénalisation des erreurs de classification.

Nous avons testé plusieurs valeurs de C réparties logarithmiquement entre $10^{-5} \quad \text{et} \quad 10^{5}$.

```{python}
# Affiche un titre pour indiquer que l'on travaille avec un noyau linéaire 
print("--- Linear kernel ---")
print("Fitting the classifier to the training set")
# Lance un chronomètre pour mesurer le temps d'exécution
t0 = time()

# Exploration du paramètre C
Cs = 10. ** np.arange(-5, 6)
scores = []
for C in Cs:
    clf = SVC(kernel="linear", C=C)
    clf.fit(X_train, y_train)
    scores.append(clf.score(X_test, y_test))

# Sélection du meilleur C
ind = np.argmax(scores)
print("Best C: {}".format(Cs[ind]))

# Visualisation des meilleures performances 
plt.figure()
plt.plot(Cs, scores)
plt.xlabel("Paramètres de régularisation C")
plt.ylabel("Scores d'apprentissage")
plt.xscale("log")
plt.tight_layout()
plt.show()
print("Best score: {}".format(np.max(scores)))

# Préparation à la prédiction
print("Predicting the people names on the testing set")
t0 = time()
```

L'étude de la performance en fonction de C montre qu'il existe une valeur optimale qui maximise le score de test. Pour des C trop faibles, le modèle est trop contraint et ne capture pas correctement les différences entre classes (sous-apprentissage). Pour des valeurs très grandes, le modèle devient trop sensible aux données d'entraînement, ce qui peut conduire à un surapprentissage.

Afin de mieux analyser le comportement du modèle, nous avons représenté l'évolution de l'erreur de prédiction (plutôt que du score). La meilleure valeur de C est indiquée par un point rouge sur le graphique.

```{python}
# Calcul de l'erreur de prédiction pour différentes valeurs de C
errors = []
for C in Cs:
    clf = SVC(kernel="linear", C=C)
    clf.fit(X_train, y_train)
    errors.append(1 - clf.score(X_test, y_test))

# Sélection du meilleur C (minimisant l'erreur)
best_ind = np.argmin(errors)
best_C = Cs[best_ind]
best_error = errors[best_ind]

plt.figure()
plt.plot(Cs, errors, marker="o")

# Visualisation de l'erreur en fonction de C
plt.scatter(best_C, best_error, color="red", s=100, zorder=5)

plt.xscale("log")
plt.xlabel("Paramètre de régularisation C")
plt.ylabel("Erreur de prédiction")
plt.title("Influence de C sur la performance")
plt.grid(True)
plt.tight_layout()
plt.show()

# Affichage des résultats
print("Best C: {}".format(Cs[best_ind]))
print("Best error: {}".format(np.min(errors)))
print("Best accuracy(score): {}".format(1 - np.min(errors)))
t0 = time()
```

Ces deux représentations sont complémentaires et mettent en évidence que :

- pour des C trop petits ($10^{-5}$), le modèle sous-apprend (*underfitting*),

- pour des C trop grands ($10^{5}$), il tend au surapprentissage (*overfitting*).  

Pour approfondir l'analyse, nous avons construit les matrices de confusion pour deux cas extrêmes : $C=1e^{-5}$ et $C=1e^{5}$.

La matrice de confusion permet de visualiser la performance du classificateur de manière détaillée. Chaque ligne correspond aux classes réelles, et chaque colonne aux classes prédites. Ainsi, elle montre non seulement le taux global de bonne classification, mais aussi quelles classes sont confondues par le modèle.

```{python}
# Matrice de confusion
# Cas 1 : SVM avec une régularisation très forte (C petit)
clf_small = SVC(kernel="linear", C=1e-5)
clf_small.fit(X_train, y_train)
y_pred_small = clf_small.predict(X_test)

cm_small = confusion_matrix(y_test, y_pred_small, labels=clf_small.classes_)
disp_small = ConfusionMatrixDisplay(confusion_matrix=cm_small, display_labels=clf_small.classes_)

# Cas 2 : SVM avec une régularisation très faible (C grand)
clf_large = SVC(kernel="linear", C=1e5)
clf_large.fit(X_train, y_train)
y_pred_large = clf_large.predict(X_test)

cm_large = confusion_matrix(y_test, y_pred_large, labels=clf_large.classes_)
disp_large = ConfusionMatrixDisplay(confusion_matrix=cm_large, display_labels=clf_large.classes_)

# Visualisation côte à côte des deux matrices
fig, axes = plt.subplots(1, 2, figsize=(12, 5))
# 0 = Donald Rumsfeld, 1 = Colin Powell
class_names = ['Donald Rumsfeld', 'Colin Powell']

disp_small = ConfusionMatrixDisplay(confusion_matrix=cm_small,
                                    display_labels=class_names)
disp_small.plot(ax=axes[0], cmap="Blues", xticks_rotation='vertical', colorbar=False)
axes[0].set_title("Confusion matrix (C=1e-5)")
disp_large = ConfusionMatrixDisplay(confusion_matrix=cm_large,
                                    display_labels=class_names)
disp_large.plot(ax=axes[1], cmap="Blues", xticks_rotation='vertical', colorbar=False)
axes[1].set_title("Confusion matrix (C=1e5)")

plt.tight_layout()
plt.show()
```

L'observation confirme les tendances générales :

- avec $C=10^{-5}$, le modèle prédit uniquement *Colin Powell* et ignore totalement *Donald Rumsfeld*,

- avec $C=10^{5}$, les deux classes sont mieux distinguées, mais quelques erreurs persistent.

Nous fournirons les mêmes résultats, mais sous forme numérique :

```{python}
print("=== Classification report (C=1e-5) ===")
print(classification_report(y_test, y_pred_small, target_names=class_names))

print("\n=== Classification report (C=1e5) ===")
print(classification_report(y_test, y_pred_large, target_names=class_names))
```

Les rapports de classification confirment que :

- pour $C = 10^{-5}$, la précision et le rappel pour *Donald Rumsfeld* sont nuls, ce qui signifie que le modèle est totalement incapable de reconnaître cette classe,

- pour $C = 10^{5}$, la performance s'améliore nettement avec une précision globale de $88 \%$, bien que quelques erreurs subsistent pour les deux individus.  

Nous comparons la précision du modèle avec le niveau de hasard (*baseline*). Ainsi, nous pouvons évaluer dans quelle mesure la valeur choisie de C améliore les résultats.

```{python}
# Prédiction avec le meilleur modèle
clf = SVC(kernel="linear", C=Cs[ind])
clf.fit(X_train, y_train)
y_pred = clf.predict(X_test)

# Mesure de temps d'exécution
print("done in %0.3fs" % (time() - t0))

# Calcul du niveau de hasard
print("Chance level : %s" % max(np.mean(y), 1. - np.mean(y)))
# Evaluation de la précision
print("Accuracy : %s" % clf.score(X_test, y_test))
```

Les résultats montrent que :

- l'entraînement du modèle prend environ $0,7$ seconde,

- le niveau de hasard est de $62,1 \%$ (c'est-à-dire la précision atteinte si l'on prédit toujours la classe majoritaire),

- la précision réelle du modèle sur les données de test est d'environ $90,5\%$, ce qui prouve l'efficacité du choix de C.

Enfin, nous avons comparé les prédictions et les coefficients appris par le classificateur pour trois cas : le meilleur C, la valeur très faible ($C=10^{-5}$) et la valeur très grande ($C=10^{5}$).

```{python}
# Evaluation qualitative des prédictions
prediction_titles = [title(y_pred[i], y_test[i], names)
                     for i in range(y_pred.shape[0])]

# Affiche galerie d'images test avec les titres générés
plot_gallery(images_test, prediction_titles)
plt.show()

# Visualisation des coefficients du modèle SVM
plt.figure()
plt.imshow(np.reshape(clf.coef_, (h, w)))
plt.show()
```

Ces visualisations mettent en évidence plusieurs points.

Pour $C = 10^{-5}$, l'image des coefficients est très floue, et seules les caractéristiques les plus importantes sont prises en compte, telles que la taille du front, les sourcils, la forme du nez et des lèvres.

Pour $C = 10^{5}$, le modèle devient très sensible et commence à détecter des motifs même là où il n’y en a pas. Par exemple, on observe beaucoup de points rouges sur l'arrière-plan des images, ce qui n'est pas pertinent, car les deux personnes peuvent être photographiées dans n'importe quelle pièce.

```{python}
# Extraction des coefficients des deux modèles (C=1e-5 et C=1e5)
coef_small = clf_small.coef_.ravel()
coef_large = clf_large.coef_.ravel()

# Visualisation côte à côte des cartes de poids
fig, axes = plt.subplots(1, 2, figsize=(10, 5))

axes[0].imshow(coef_small.reshape(h, w), cmap=plt.cm.seismic, interpolation="nearest")
axes[0].set_title("Coefficients (C=1e-5)")

axes[1].imshow(coef_large.reshape(h, w), cmap=plt.cm.seismic, interpolation="nearest")
axes[1].set_title("Coefficients (C=1e5)")

plt.tight_layout()
plt.show()
```

Cette comparaison illustre clairement la différence entre sous-apprentissage et surapprentissage et montre comment le paramètre C contrôle la sensibilité du modèle aux caractéristiques des données.

### Question 5 - Ajout de variables de nuisance

Dans cette partie, nous étudions l'effet de l'ajout de variables de nuisance, c'est-à-dire des variables aléatoires sans lien avec la tâche de classification. L'objectif est d'augmenter artificiellement la dimension du problème, tout en conservant le même nombre d'exemples d'apprentissage, afin d'analyser l'impact sur la capacité de généralisation du modèle. 

Nous commençons par définir une fonction permettant d'entraîner un SVM à noyau linéaire en validation croisée.

```{python}
# Fonction de validation croisée avec SVM linéaire 
def run_svm_cv(_X, _y):
    _indices = np.random.permutation(_X.shape[0])
    _train_idx, _test_idx = _indices[:_X.shape[0] // 2], _indices[_X.shape[0] // 2:]
    _X_train, _X_test = _X[_train_idx, :], _X[_test_idx, :]
    _y_train, _y_test = _y[_train_idx], _y[_test_idx]

    _parameters = {'kernel': ['linear'], 'C': list(np.logspace(-3, 3, 5))}
    _svr = svm.SVC()
    _clf_linear = GridSearchCV(_svr, _parameters)
    _clf_linear.fit(_X_train, _y_train)

    print('Generalization score for linear kernel: %s, %s \n' %
          (_clf_linear.score(_X_train, _y_train), _clf_linear.score(_X_test, _y_test)))

# Évaluation sans variables de nuisance 
print("Score sans variable de nuisance")
run_svm_cv(X, y)

# Ajout de 300 variables aléatoires
print("Score avec variable de nuisance")
n_features = X.shape[1]
sigma = 1
noise = sigma * np.random.randn(n_samples, 300, ) 
# Avec des coefficients gaussiens de sigma-type
X_noisy = np.concatenate((X, noise), axis=1)
X_noisy = X_noisy[np.random.permutation(X.shape[0])]
np.random.shuffle(X_noisy.T)

run_svm_cv(X_noisy, y)
```

Les résultats nous montrent que sans variables de nuisance, le SVM obtient une précision d'environ $0.89$ sur l'échantillon de test. En revanche, après l'ajout de $300$ variables aléatoires, la précision chute à environ $0.48$, soit proche du niveau du hasard. 

Ce contraste illustre bien que l'ajout de dimensions inutiles entraine un surapprentissage et dégrade sa capacité de généralisation.

Pour rendre l'analyse plus robuste, nous fixons un `random_state=42` et faison varier le nombre de variables de nuisance.

```{python}
# Fonction de validation croisée avec SVM linéaire
def run_svm_cv(_X, _y, random_state=42):
    rng = np.random.RandomState(random_state)
    indices = rng.permutation(_X.shape[0])
    train_idx, test_idx = indices[:_X.shape[0] // 2], indices[_X.shape[0] // 2:]
    X_train, X_test = _X[train_idx, :], _X[test_idx, :]
    y_train, y_test = _y[train_idx], _y[test_idx]

    parameters = {'kernel': ['linear'], 'C': list(np.logspace(-3, 3, 5))}
    svr = svm.SVC()
    clf = GridSearchCV(svr, parameters)
    clf.fit(X_train, y_train)

    return clf.score(X_test, y_test)

# Expérience : impact du bruit sur la précision
noise_dims = np.arange(10, 1001, 25)
test_scores = []

rng = np.random.RandomState(42)
for n_noise in noise_dims:
    noise = rng.randn(X.shape[0], n_noise)
    X_aug = np.concatenate((X, noise), axis=1)
    score = run_svm_cv(X_aug, y)
    test_scores.append(score)

# Visualisation des résultats
plt.figure(figsize=(10,6))
plt.plot(noise_dims, test_scores, color='red')
plt.title("Influence du nombre de variables de nuisance sur l'accuracy")
plt.xlabel("Nombre de variables de nuisance")
plt.ylabel("Accuracy sur l'échantillon de test")
plt.grid(True)
plt.show()
```

Le graphique obtenu montre que pour un faible nombre de variables de nuisance, la précision reste relativement élevée, ce qui prouve que le modèle continue à exploiter les caractéristiques pertinentes. Mais à mesure que le nombre de dimensions bruitées augmente, la performance se détériore progressivement, confirmant l'effet négatif de variables non informatives.

Ainsi, cette expérience illustre un phénomène classique : l'ajout de bruit augmente la complexité du problème sans apporter d'information utile, ce qui dégrade la généralisation du modèle.

### Question 6 - PCA

si on utilise les codes 

```text
import time as tm
print("Score apres reduction de dimension")

n_components = 10  # jouer avec ce parametre
pca = PCA(n_components=n_components).fit(X_noisy)
X_pca = pca.transform(X_noisy)

# timing
t0 = tm.time()
run_svm_cv(X_pca, y)
elapsed = tm.time() - t0

print(f"Nombre de composantes PCA: {n_components}")
print(f"Temps de calcul: {elapsed:.3f} secondes")
```
sortie de code :


Score apres reduction de dimension
Generalization score for linear kernel: 0.6526315789473685, 0.5894736842105263 

Nombre de composantes PCA: 2
Temps de calcul: 173.942 secondes

Score apres reduction de dimension
Generalization score for linear kernel: 0.6421052631578947, 0.6 

Nombre de composantes PCA: 10
Temps de calcul: 733.982 secondes

Score apres reduction de dimension
Generalization score for linear kernel: 0.6842105263157895, 0.6052631578947368 

Nombre de composantes PCA: 20
Temps de calcul: 1357.474 secondes

Score apres reduction de dimension
Generalization score for linear kernel: 0.8789473684210526, 0.5736842105263158 

Score apres reduction de dimension
Generalization score for linear kernel: 0.7736842105263158, 0.5473684210526316 

Nombre de composantes PCA: 50
Temps de calcul: 2886.425 secondes

Nombre de composantes PCA: 100
Temps de calcul: 0.145 secondes

Score apres reduction de dimension
Generalization score for linear kernel: 0.868421052631579, 0.631578947368421 

Nombre de composantes PCA: 150
Temps de calcul: 0.095 secondes

Score apres reduction de dimension
Generalization score for linear kernel: 1.0, 0.5 

Nombre de composantes PCA: 200
Temps de calcul: 0.101 secondes

### Question 7 - Biais dans le prétaitement
